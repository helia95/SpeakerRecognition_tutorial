{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchaudio.datasets.LIBRISPEECH(root= , url = 'train-clean-100', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open('/cas/DeepLearn/elperu/tmp/speech_datasets/LibriSpeech/README.TXT', 'r')\n",
    "#readme = f.readlines()\n",
    "#readme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset structure\n",
    "100 hours of speech, different speakers reads chapters of books"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "base_path|\n",
    "        - speaker_id_1|\n",
    "                        - ch_1\n",
    "                        - ch_2\n",
    "        - speaker_id_2|\n",
    "        ...\n",
    "        \n",
    "        \n",
    "Average duration abuot 10 s per utterance\n",
    "\n",
    "\n",
    "Collect 20 utterance per speaker and use it for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = '/cas/DeepLearn/elperu/tmp/speech_datasets/LibriSpeech/train-clean-100/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of speaker: 251\n"
     ]
    }
   ],
   "source": [
    "speakers = os.listdir(dataroot)\n",
    "print(f'Number of speaker: {len(speakers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 7312 has 26\n",
      "\n",
      "Number of valid speakers: 250\n"
     ]
    }
   ],
   "source": [
    "# Check speakers to keep\n",
    "valid_spks = []   # spks with more than x utternaces\n",
    "n_utterances_per_spk = 40\n",
    "n_test_utterances_per_spk = 3\n",
    "\n",
    "for speaker in speakers:\n",
    "    chapters = os.listdir(os.path.join(dataroot, speaker))\n",
    "    \n",
    "    n_utters = 0\n",
    "    \n",
    "    for ch in chapters:\n",
    "        n_utters += len([f for f in os.listdir(os.path.join(dataroot, speaker, ch)) if f.endswith('.flac')])\n",
    "   \n",
    "    if n_utters < n_utterances_per_spk + n_test_utterances_per_spk:\n",
    "        print(f'Speaker {speaker} has {n_utters}')\n",
    "    else: \n",
    "        valid_spks.append(speaker)\n",
    "        \n",
    "print('\\nNumber of valid speakers: {}'.format(len(valid_spks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choose N speakers for test set\n",
    "N = 50\n",
    "\n",
    "test_speakers = random.sample(valid_spks, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_speakers = [s for s in valid_spks if s not in test_speakers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_dataframe(spk_list, train_flag=True):\n",
    "    tot_rows = []\n",
    "    train_test_rows = []   # files used to test the identification system\n",
    "    \n",
    "    for speaker in spk_list:\n",
    "        chapters = os.listdir(os.path.join(dataroot, speaker))\n",
    "    \n",
    "        utterances = []\n",
    "        for ch in chapters:\n",
    "            utterances.extend(glob.glob(os.path.join(dataroot, speaker, ch, '*.flac')))\n",
    "\n",
    "        for utter in utterances[:n_utterances_per_spk]:\n",
    "            tot_rows.append([speaker, ch, os.path.abspath(os.path.join(dataroot, speaker, ch, utter))])\n",
    "            \n",
    "        if train_flag:\n",
    "            for utter in utterances[n_utterances_per_spk: (n_utterances_per_spk + n_test_utterances_per_spk)]:\n",
    "                train_test_rows.append([speaker, ch, os.path.abspath(os.path.join(dataroot, speaker, ch, utter))])\n",
    "            \n",
    "    assert len(tot_rows) == n_utterances_per_spk * len(spk_list)\n",
    "    \n",
    "    if train_flag:\n",
    "        assert len(train_test_rows) == n_test_utterances_per_spk * len(spk_list)\n",
    "    \n",
    "    if not train_flag:\n",
    "        \n",
    "        return pd.DataFrame(tot_rows,  columns=['spk_id', 'ch_id', 'utter_path'])\n",
    "    \n",
    "    else:\n",
    "        return pd.DataFrame(tot_rows,  columns=['spk_id', 'ch_id', 'utter_path']), pd.DataFrame(train_test_rows,  columns=['spk_id', 'ch_id', 'utter_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = config_dataframe(test_speakers, train_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_train_test = config_dataframe(train_speakers, train_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store config files as csv\n",
    "out_path = '/cas/DeepLearn/elperu/tmp/speech_datasets/LibriSpeech/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(os.path.join(out_path, 'test_config.csv'), index=False)\n",
    "df_train.to_csv(os.path.join(out_path, 'train_config.csv'), index=False)\n",
    "df_train_test.to_csv(os.path.join(out_path, 'identification_test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:speaker]",
   "language": "python",
   "name": "conda-env-speaker-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
